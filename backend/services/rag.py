from typing import List, Dict
import logging
from config.settings import settings
from services.vector_db import VectorDBService
from services.embedding import EmbeddingService
from models.chat import ChatRequest, ChatResponse, DocumentChunk

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        self.vector_db = VectorDBService()
        self.embedding_service = EmbeddingService()
        logger.info("Initialized RAG Service")

    def get_relevant_context(self, query: str, top_k: int = 5, filters: Dict = {}) -> List[DocumentChunk]:
        """Retrieve relevant context from the vector database"""
        try:
            query_embedding = self.embedding_service.encode_single(query)
            search_results = self.vector_db.search(
                query_vector=query_embedding,
                top_k=top_k,
                filters=filters
            )

            context_chunks = []
            for result in search_results:
                chunk = DocumentChunk(
                    id=result["id"],
                    content=result["content"],
                    document_id=result["document_id"],
                    chunk_index=result["chunk_index"],
                    metadata=result["metadata"]
                )
                context_chunks.append(chunk)

            logger.info(f"Retrieved {len(context_chunks)} relevant chunks for query")
            return context_chunks
        except Exception as e:
            logger.error(f"Error retrieving context: {e}")
            return []

    def generate_response(self, query: str, context_chunks: List[DocumentChunk], history: List = []) -> str:
        """Generate a response using the LLM with provided context"""
        try:
            # Prepare context from chunks
            context_text = "\n\n".join([chunk.content for chunk in context_chunks])

            # Prepare the prompt for the LLM
            if context_text:
                prompt = f"""
                You are an AI assistant for the AI-Native Textbook on Physical AI and Humanoid Robotics.
                Use the following context to answer the user's question.
                If the context doesn't contain the information needed, say so politely.

                Context:
                {context_text}

                User's question: {query}

                Answer:
                """
            else:
                prompt = f"""
                You are an AI assistant for the AI-Native Textbook on Physical AI and Humanoid Robotics.
                The user asked: {query}

                I couldn't find specific information about this in the textbook content.
                Please provide a general response or ask the user to clarify their question.
                """

            # In a real implementation, you would call an LLM API here
            # For this example, we'll return a simulated response
            response = self._call_llm(prompt)
            return response

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return "I encountered an error processing your request. Please try again."

    def _call_llm(self, prompt: str) -> str:
        """Call the LLM - this is a placeholder that would be replaced with actual LLM call"""
        # This is a simulation - in production, you would call an actual LLM
        # like OpenAI GPT, Anthropic Claude, or an open-source model
        return f"This is a simulated response based on the context. In a real implementation, this would be generated by an LLM using: {prompt[:100]}..."

    def chat(self, request: ChatRequest) -> ChatResponse:
        """Main chat method that orchestrates RAG process"""
        try:
            # Get relevant context from vector database
            context_chunks = self.get_relevant_context(
                query=request.message,
                top_k=request.context_limit
            )

            # Generate response using context
            response_text = self.generate_response(
                query=request.message,
                context_chunks=context_chunks,
                history=request.history
            )

            # Extract sources from context chunks
            sources = [chunk.metadata.get('source', f'Document {chunk.document_id}') for chunk in context_chunks]

            # Create response object
            response = ChatResponse(
                response=response_text,
                sources=list(set(sources))  # Remove duplicates
            )

            logger.info(f"Generated response for query: {request.message[:50]}...")
            return response

        except Exception as e:
            logger.error(f"Error in chat method: {e}")
            return ChatResponse(
                response="I encountered an error processing your request. Please try again.",
                sources=[]
            )